# -*- coding: utf-8 -*-
"""KNN,K-means.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w9xs6a-Av4n6I6td87phkVBNvia9fYG0
"""

import numpy as np
import matplotlib.pyplot as plt

import numpy as np
from collections import Counter

def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2) ** 2))


def knn(X_train, y_train, X_test, k=3):
    predictions = []

    for test_point in X_test:

        distances = []
        for i, train_point in enumerate(X_train):
            dist = euclidean_distance(test_point, train_point)
            distances.append((dist, i))

        distances.sort(key=lambda x: x[0])
        nearest_neighbors_indices = [distances[i][1] for i in range(k)]


        nearest_neighbors_labels = [y_train[i] for i in nearest_neighbors_indices]


        most_common = Counter(nearest_neighbors_labels).most_common(1)
        predictions.append(most_common[0][0])

    return predictions



X_train = np.array([[2.5, 3.4], [1.3,2.3], [3.1, 4], [5.4, 2.1], [6.2, 3.2], [5.8, 4.2], [3.5, 5.1], [4.5,3.8], [6.1,2.8], [2.0, 3]])
y_train = np.array([0,0,0,1,1,1,0, 1, 1, 0])


X_test = np.array([[5.3,2.5]])


k = 3
predictions = knn(X_train, y_train, X_test, k)
print(predictions)

import numpy as np

def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2) ** 2))

def kmeans(X, k=3, max_iters=100):
    # Step 1: Initialize centroids randomly from data points
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(max_iters):
        # Step 2: Assign each point to the nearest centroid
        labels = []
        for point in X:
            distances = [euclidean_distance(point, centroid) for centroid in centroids]
            labels.append(np.argmin(distances))  # Assign the point to the nearest centroid

        labels = np.array(labels)

        # Step 3: Update centroids
        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

        # Step 4: Check for convergence (if centroids don't change, break out of loop)
        if np.all(centroids == new_centroids):
            break

        centroids = new_centroids

    return centroids, labels

# Training data
X_train = np.array([[2.5, 3.4], [1.3, 2.3], [3.1, 4], [5.4, 2.1], [6.2, 3.2],
                    [5.8, 4.2], [3.5, 5.1], [4.5, 3.8], [6.1, 2.8], [2.0, 3]])

# Apply K-Means clustering
k = 3
centroids, labels = kmeans(X_train, k)

print("Centroids:", centroids)
print("Labels for each data point:", labels)

import numpy as np
import matplotlib.pyplot as plt

def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2) ** 2))

def kmeans(X, k=3, max_iters=100):
    # Step 1: Initialize centroids randomly from data points
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(max_iters):
        # Step 2: Assign each point to the nearest centroid
        labels = []
        for point in X:
            distances = [euclidean_distance(point, centroid) for centroid in centroids]
            labels.append(np.argmin(distances))  # Assign the point to the nearest centroid

        labels = np.array(labels)

        # Step 3: Update centroids
        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

        # Step 4: Check for convergence (if centroids don't change, break out of loop)
        if np.all(centroids == new_centroids):
            break

        centroids = new_centroids

    return centroids, labels

# Training data
X_train = np.array([[2.5, 3.4], [1.3, 2.3], [3.1, 4], [5.4, 2.1], [6.2, 3.2],
                    [5.8, 4.2], [3.5, 5.1], [4.5, 3.8], [6.1, 2.8], [2.0, 3]])

# Apply K-Means clustering
k = 3
centroids, labels = kmeans(X_train, k)

# Plotting
plt.figure(figsize=(8, 6))

# Plot data points, colored by their cluster label
for i in range(k):
    plt.scatter(X_train[labels == i][:, 0], X_train[labels == i][:, 1], label=f'Cluster {i+1}')

# Plot centroids
plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', color='black', s=200, label='Centroids')

# Add labels and title
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-Means Clustering Results')
plt.legend()

# Show the plot
plt.grid(True)
plt.show()

import numpy as np
from collections import Counter

def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2) ** 2))


def knn(X_train, y_train, X_test, k=3):
    predictions = []

    for test_point in X_test:

        distances = []
        for i, train_point in enumerate(X_train):
            dist = euclidean_distance(test_point, train_point)
            distances.append((dist, i))

        distances.sort(key=lambda x: x[0])
        nearest_neighbors_indices = [distances[i][1] for i in range(k)]


        nearest_neighbors_labels = [y_train[i] for i in nearest_neighbors_indices]


        most_common = Counter(nearest_neighbors_labels).most_common(1)
        predictions.append(most_common[0][0])

    return predictions



X_train = np.array([[2.5, 3.4], [1.3,2.3], [3.1, 4], [5.4, 2.1], [6.2, 3.2], [5.8, 4.2], [3.5, 5.1], [4.5,3.8], [6.1,2.8], [2.0, 3]])
y_train = np.array([0,0,0,1,1,1,0, 1, 1, 0])


X_test = np.array([[2.3,2.5]])


k = 3
predictions = knn(X_train, y_train, X_test, k)
print(predictions)